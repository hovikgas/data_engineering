{"cells":[{"cell_type":"code","source":["import pandas as pd\nimport numpy as np\nimport os\nfrom pyspark.sql.types import StringType, FloatType, TimestampType, StructType, StructField"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f2d34ed9-d38a-4a1d-a3f0-950350b0ad3c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Read in data. In production, this would be handled by AutoLoader, or some other batch/streaming data ingestion alternative if Databricks is not the platform of choice."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"63f3877d-4cca-45f5-8a07-19e1e0fe3b19","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["raw_data = pd.read_parquet(f'file:{os.getcwd()}/data/sample.parquet')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2cf10af6-e69f-493d-82c6-81a8421fbecc","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["If we were using AutoLoader, the following example would create a structured stream of the data, with the accompanied schema, and write it out to a raw table. After this we would be able to import the latest data from the \"bronze,\" or raw table, to continue our ETL process. Normally, I would have this be in a separate notebook/script, for increased modularity, but since we are simply using the sample parquet file for now, I'll leave here out of convenience. Thus, the rest of the transformations seen below in Pandas would not necessarily apply in this structured streaming scenario."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2c2e199c-40a6-489d-b576-60a3575e5ec8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# schema = StructType(\n#     [\n#         StructField('time', TimestampType(), True),\n#         StructField('value', FloatType(), True),\n#         StructField('field', StringType(), True),\n#         StructField('robot_id', StringType(), True),\n#         StructField('run_uuid', StringType(), True),\n#         StructField('sensor_type', StringType(), True)\n#     ]\n# )\n\n# raw_stream = (\n#     spark.readStream.format(\"cloudFiles\")\n#     .schema(schema)\n#     .option(\"cloudFiles.format\", \"parquet\")\n#     .load(f'file:{os.getcwd()}/data/sample.parquet')\n# )\n\n#write stream to delta table\n# (\n#     raw_stream.writeStream\n#     .format(\"delta\")\n#     .outputMode(\"append\")\n#     .trigger(availableNow=True)\n#     .option(\"mergeSchema\", \"true\")\n#     .option(\"checkpointLocation\", f'file:{os.getcwd()}/data/')\n#     .table('activity_db.raw_telemetry')\n#     .awaitTermination()\n# )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7bce1a3b-e1ad-4695-a7e5-4b0e27b1b265","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Check Data Types"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9a76e724-873f-48a9-8137-15f3bdd61b20","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["raw_data.dtypes"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e936d608-8669-41de-89ad-762d02c336b7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[3]: time            object\nvalue          float64\nfield           object\nrobot_id         int64\nrun_uuid       float64\nsensor_type     object\ndtype: object","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[3]: time            object\nvalue          float64\nfield           object\nrobot_id         int64\nrun_uuid       float64\nsensor_type     object\ndtype: object"]}}],"execution_count":0},{"cell_type":"markdown","source":["Fix Data Types. Again, in the production, the schema enforcement would be handled by the"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eb07662c-4072-420a-8c12-2903beffdb42","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["raw_data['time'] = raw_data['time'].astype('datetime64')\nraw_data['robot_id'] = raw_data['robot_id'].astype('str')\nraw_data['run_uuid'] = raw_data['run_uuid'].astype('str')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"18a5cff0-4b14-48bc-a33f-52421c46db85","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Feature Engineering -- robot_id<->encoder variable"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a8a9c30d-3510-4b53-b7b9-5c39fbf368c6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["raw_data['combined'] = raw_data['field'] + \"_\" + raw_data['robot_id']"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"27a320a9-031e-404f-8feb-0ad6c8936dd7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Convert from Long to Wide"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b99e02db-34cc-4799-9cf2-e81288375563","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["wide_data = raw_data.pivot_table(index='time', columns='combined', values='value')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"29b026da-3065-43b2-8dff-bcd7f33dc7ab","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Interpolate the Values Missing Between Timestamps\n\nGiven the fact that the encoder and load cell sensors run independently and aren't in sync, this is not the best method overall. We are sacrificing accuracy for expedience. In this case, matching timestamps between features would likely be the better route, but would require more time and resources to do well. There would still be a risk of losing data, depending on the precision of the time resolution measurements, as well as the latency between the two robots' sensors. Given the current constraints, interpolation was chosen because it is simpler and more straightforward."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"100dfb0c-edb0-427d-922e-e4e4765230a8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["intpol_data = wide_data.interpolate()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"64114f97-9cc4-41da-83a2-07b0b8535d9b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Recreate the time column to then be able to engineer a once-differenced time column for use in the velocity calculations"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7540fce6-8d33-49e3-85b8-202b466dd6d1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["intpol_data['time_col'] = intpol_data.index"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0bbf4041-1799-446f-b123-b50df66dd97a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["intpol_data['time_diff'] = intpol_data['time_col'].diff().dropna()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5127ed8f-dcca-4c94-b3b9-1b42f87b7f2e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Because the time_diff column is in a Timedelta format, we need it as a numeric type to be able to do the calculations, so we convert it to the total duration in milliseconds"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"91471897-379f-44a5-908e-efc015535688","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["intpol_data['time_diff_ms'] = intpol_data['time_diff'].apply(lambda x: x.total_seconds()*1000)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"89923a12-1311-4b9f-ac10-362a8d1b11dc","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Compute the velocity, and accelaration values by taking the change in the cartesian coordinates over time, as well as the change in velocity over time."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9599b71d-2f70-4122-829c-26c7d83c65e5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Compute velocity values\nintpol_data['vx_1'] = intpol_data['x_1'].diff() / intpol_data['time_diff_ms']\nintpol_data['vy_1'] = intpol_data['y_1'].diff() / intpol_data['time_diff_ms']\nintpol_data['vz_1'] = intpol_data['z_1'].diff() / intpol_data['time_diff_ms']\nintpol_data['vx_2'] = intpol_data['x_2'].diff() / intpol_data['time_diff_ms']\nintpol_data['vy_2'] = intpol_data['y_2'].diff() / intpol_data['time_diff_ms']\nintpol_data['vz_2'] = intpol_data['z_2'].diff() / intpol_data['time_diff_ms']"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6801ccb2-13e3-4d38-b493-ee802c38c310","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Compute acceleration values\nintpol_data['ax_1'] = intpol_data['vx_1'].diff() / intpol_data['time_diff_ms']\nintpol_data['ay_1'] = intpol_data['vy_1'].diff() / intpol_data['time_diff_ms']\nintpol_data['az_1'] = intpol_data['vz_1'].diff() / intpol_data['time_diff_ms']\nintpol_data['ax_2'] = intpol_data['vx_2'].diff() / intpol_data['time_diff_ms']\nintpol_data['ay_2'] = intpol_data['vy_2'].diff() / intpol_data['time_diff_ms']\nintpol_data['az_2'] = intpol_data['vz_2'].diff() / intpol_data['time_diff_ms']"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1e502d8a-25a9-4739-b1da-2e866a8bc13e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Compute the total velocities, accelarations, and force by taking the root of the sum of the squares of each velocity, acceleration, and force vector."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"365d9707-b842-43d7-9927-3e297483690d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Compute total velocities and accelerations\nintpol_data['v1'] = np.sqrt(intpol_data['vx_1']**2 + intpol_data['vy_1']**2 + intpol_data['vz_1']**2)\nintpol_data['v2'] = np.sqrt(intpol_data['vx_2']**2 + intpol_data['vy_2']**2 + intpol_data['vz_2']**2)\nintpol_data['a1'] = np.sqrt(intpol_data['ax_1']**2 + intpol_data['ay_1']**2 + intpol_data['az_1']**2)\nintpol_data['a2'] = np.sqrt(intpol_data['ax_2']**2 + intpol_data['ay_2']**2 + intpol_data['az_2']**2)\n\n# Same thing for total force\nintpol_data['f1'] = np.sqrt(intpol_data['fx_1']**2 + intpol_data['fy_1']**2 + intpol_data['fz_1']**2)\nintpol_data['f2'] = np.sqrt(intpol_data['fx_2']**2 + intpol_data['fy_2']**2 + intpol_data['fz_2']**2)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9af7b434-01d6-47d1-83fa-7ae51147d789","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Compute the Runtime Statistics by run_uuid by first grouping the data, then calculating the min/max for start/stop time and corresponding total runtime."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1f4371fd-92da-474a-8ed1-f8986cfe1b24","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["grouped_df = raw_data.groupby('run_uuid')\nstats_df_start = grouped_df.agg({'time': 'min'})\nstats_df_stop = grouped_df.agg({'time': 'max'})\nstats_df_stop['total_runtime'] = stats_df_stop['time'] - stats_df_start['time']\nstats_df_distance = grouped_df.agg({'value':'sum'})"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bade25cb-5bfc-4a0d-accf-b563af5c9404","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Merging all the dataframes together and renaming their columns for the final runtime statistics dataframe"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b6801ad6-b7f5-42b8-a7a6-7ef68a89dee5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["stats_df_both = stats_df_start.merge(stats_df_stop, on='run_uuid')\nfinal_stats_df = stats_df_both.merge(stats_df_distance, on = 'run_uuid')\nfinal_stats_df.rename(columns = {'time_x':'start_time', 'time_y':'stop_time', 'value':'total_distance'})"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"21e16774-15b4-4064-bb0e-b374396add01","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>start_time</th>\n      <th>stop_time</th>\n      <th>total_runtime</th>\n      <th>total_distance</th>\n    </tr>\n    <tr>\n      <th>run_uuid</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1.2405186538561671e+19</th>\n      <td>2022-11-23 20:40:00.003</td>\n      <td>2022-11-23 20:41:17.630</td>\n      <td>0 days 00:01:17.627000</td>\n      <td>-6.829650e+05</td>\n    </tr>\n    <tr>\n      <th>6.176976534744076e+18</th>\n      <td>2022-11-23 20:40:00.007</td>\n      <td>2022-11-23 20:49:59.999</td>\n      <td>0 days 00:09:59.992000</td>\n      <td>1.858821e+08</td>\n    </tr>\n    <tr>\n      <th>7.58229308099147e+18</th>\n      <td>2022-11-23 20:40:00.001</td>\n      <td>2022-11-23 20:49:59.998</td>\n      <td>0 days 00:09:59.997000</td>\n      <td>2.408971e+08</td>\n    </tr>\n    <tr>\n      <th>8.910095844186657e+18</th>\n      <td>2022-11-23 20:40:00.005</td>\n      <td>2022-11-23 20:49:59.999</td>\n      <td>0 days 00:09:59.994000</td>\n      <td>1.200283e+08</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>start_time</th>\n      <th>stop_time</th>\n      <th>total_runtime</th>\n      <th>total_distance</th>\n    </tr>\n    <tr>\n      <th>run_uuid</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1.2405186538561671e+19</th>\n      <td>2022-11-23 20:40:00.003</td>\n      <td>2022-11-23 20:41:17.630</td>\n      <td>0 days 00:01:17.627000</td>\n      <td>-6.829650e+05</td>\n    </tr>\n    <tr>\n      <th>6.176976534744076e+18</th>\n      <td>2022-11-23 20:40:00.007</td>\n      <td>2022-11-23 20:49:59.999</td>\n      <td>0 days 00:09:59.992000</td>\n      <td>1.858821e+08</td>\n    </tr>\n    <tr>\n      <th>7.58229308099147e+18</th>\n      <td>2022-11-23 20:40:00.001</td>\n      <td>2022-11-23 20:49:59.998</td>\n      <td>0 days 00:09:59.997000</td>\n      <td>2.408971e+08</td>\n    </tr>\n    <tr>\n      <th>8.910095844186657e+18</th>\n      <td>2022-11-23 20:40:00.005</td>\n      <td>2022-11-23 20:49:59.999</td>\n      <td>0 days 00:09:59.994000</td>\n      <td>1.200283e+08</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Save the data into one database, but as 2 separate tables."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0ba9120f-0130-4246-aa7d-3de33a787d26","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark_stats_df = spark.createDataFrame(final_stats_df)\nspark_stats_df.write.mode('append').saveAsTable('activity_db.runtime_stats')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eba08a44-e3eb-49df-9071-509eea366704","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark_intpol_df = spark.createDataFrame(intpol_data)\nspark_intpol_df.write.mode('append').saveAsTable('activity_db.sensor_telemetry')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3f566632-6686-40e1-996f-c3ca43a1b28e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"data_engineering_pandas","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":895628526448253}},"nbformat":4,"nbformat_minor":0}
